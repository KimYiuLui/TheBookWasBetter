{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\KimYiuLui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine, Table, Column, MetaData, DateTime, Float, Integer, String, ARRAY, JSON\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import relationship, backref, sessionmaker, joinedload\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from rake_nltk import Rake\n",
    "import numpy as np\n",
    "import re, string\n",
    "import nltk\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from nltk.tokenize import word_tokenize \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "extraWords= ['whence', 'here', 'show', 'were', 'why',\"'s'\",\"n't\",\"'ve\", 'n’t', 'the', 'whereupon', 'not', 'more', 'how', 'eight', 'indeed', 'i', 'only', 'via', 'nine', 're', 'themselves', 'almost', 'to', 'already', 'front', 'least', 'becomes', 'thereby', 'doing', 'her', 'together', 'be', 'often', 'then', 'quite', 'less', 'many', 'they', 'ourselves', 'take', 'its', 'yours', 'each', 'would', 'may', 'namely', 'do', 'whose', 'whether', 'side', 'both', 'what', 'between', 'toward', 'our', 'whereby', \"'m\", 'formerly', 'myself', 'had', 'really', 'call', 'keep', \"'re\", 'hereupon', 'can', 'their', 'eleven', '’m', 'even', 'around', 'twenty', 'mostly', 'did', 'at', 'an', 'seems', 'serious', 'against', \"n't\", 'except', 'has', 'five', 'he', 'last', '‘ve', 'because', 'we', 'himself', 'yet', 'something', 'somehow', '‘m', 'towards', 'his', 'six', 'anywhere', 'us', '‘d', 'thru', 'thus', 'which', 'everything', 'become', 'herein', 'one', 'in', 'although', 'sometime', 'give', 'cannot', 'besides', 'across', 'noone', 'ever', 'that', 'over', 'among', 'during', 'however', 'when', 'sometimes', 'still', 'seemed', 'get', \"'ve\", 'him', 'with', 'part', 'beyond', 'everyone', 'same', 'this', 'latterly', 'no', 'regarding', 'elsewhere', 'others', 'moreover', 'else', 'back', 'alone', 'somewhere', 'are', 'will', 'beforehand', 'ten', 'very', 'most', 'three', 'former', '’re', 'otherwise', 'several', 'also', 'whatever', 'am', 'becoming', 'beside', '’s', 'nothing', 'some', 'since', 'thence', 'anyway', 'out', 'up', 'well', 'it', 'various', 'four', 'top', '‘s', 'than', 'under', 'might', 'could', 'by', 'too', 'and', 'whom', '‘ll', 'say', 'therefore', \"'s\", 'other', 'throughout', 'became', 'your', 'put', 'per', \"'ll\", 'fifteen', 'must', 'before', 'whenever', 'anyone', 'without', 'does', 'was', 'where', 'thereafter', \"'d\", 'another', 'yourselves', 'n‘t', 'see', 'go', 'wherever', 'just', 'seeming', 'hence', 'full', 'whereafter', 'bottom', 'whole', 'own', 'empty', 'due', 'behind', 'while', 'onto', 'wherein', 'off', 'again', 'a', 'two', 'above', 'therein', 'sixty', 'those', 'whereas', 'using', 'latter', 'used', 'my', 'herself', 'hers', 'or', 'neither', 'forty', 'thereupon', 'now', 'after', 'yourself', 'whither', 'rather', 'once', 'from', 'until', 'anything', 'few', 'into', 'such', 'being', 'make', 'mine', 'please', 'along', 'hundred', 'should', 'below', 'third', 'unless', 'upon', 'perhaps', 'ours', 'but', 'never', 'whoever', 'fifty', 'any', 'all', 'nobody', 'there', 'have', 'anyhow', 'of', 'seem', 'down', 'is', 'every', '’ll', 'much', 'none', 'further', 'me', 'who', 'nevertheless', 'about', 'everywhere', 'name', 'enough', '’d', 'next', 'meanwhile', 'though', 'through', 'on', 'first', 'been', 'hereby', 'if', 'move', 'so', 'either', 'amongst', 'for', 'twelve', 'nor', 'she', 'always', 'these', 'as', '’ve', 'amount', '‘re', 'someone', 'afterwards', 'you', 'nowhere', 'itself', 'done', 'hereafter', 'within', 'made', 'ca', 'them', 'her', 'during', 'among', 'thereafter', 'only', 'hers', 'in', 'none', 'with', 'un', 'put', 'hence', 'each', 'would', 'have', 'to', 'itself', 'that', 'seeming', 'hereupon', 'someone', 'eight', 'she', 'forty', 'much', 'throughout', 'less', 'was', 'interest', 'elsewhere', 'already', 'whatever', 'or', 'seem', 'fire', 'however', 'keep', 'detail', 'both', 'yourselves', 'indeed', 'enough', 'too', 'us', 'wherein', 'himself', 'behind', 'everything', 'part', 'made', 'thereupon', 'for', 'nor', 'before', 'front', 'sincere', 'really', 'than', 'alone', 'doing', 'amongst', 'across', 'him', 'another', 'some', 'whoever', 'four', 'other', 'latterly', 'off', 'sometime', 'above', 'often', 'herein', 'am', 'whereby', 'although', 'who', 'should', 'amount', 'anyway', 'else', 'upon', 'this', 'when', 'we', 'few', 'anywhere', 'will', 'though', 'being', 'fill', 'used', 'full', 'thru', 'call', 'whereafter', 'various', 'has', 'same', 'former', 'whereas', 'what', 'had', 'mostly', 'onto', 'go', 'could', 'yourself', 'meanwhile', 'beyond', 'beside', 'ours', 'side', 'our', 'five', 'nobody', 'herself', 'is', 'ever', 'they', 'here', 'eleven', 'fifty', 'therefore', 'nothing', 'not', 'mill', 'without', 'whence', 'get', 'whither', 'then', 'no', 'own', 'many', 'anything', 'etc', 'make', 'from', 'against', 'ltd', 'next', 'afterwards', 'unless', 'while', 'thin', 'beforehand', 'by', 'amoungst', 'you', 'third', 'as', 'those', 'done', 'becoming', 'say', 'either', 'doesn', 'twenty', 'his', 'yet', 'latter', 'somehow', 'are', 'these', 'mine', 'under', 'take', 'whose', 'others', 'over', 'perhaps', 'thence', 'does', 'where', 'two', 'always', 'your', 'wherever', 'became', 'which', 'about', 'but', 'towards', 'still', 'rather', 'quite', 'whether', 'somewhere', 'might', 'do', 'bottom', 'until', 'km', 'yours', 'serious', 'find', 'please', 'hasnt', 'otherwise', 'six', 'toward', 'sometimes', 'of', 'fifteen', 'eg', 'just', 'a', 'me', 'describe', 'why', 'an', 'and', 'may', 'within', 'kg', 'con', 're', 'nevertheless', 'through', 'very', 'anyhow', 'down', 'nowhere', 'now', 'it', 'cant', 'de', 'move', 'hereby', 'how', 'found', 'whom', 'were', 'together', 'again', 'moreover', 'first', 'never', 'below', 'between', 'computer', 'ten', 'into', 'see', 'everywhere', 'there', 'neither', 'every', 'couldnt', 'up', 'several', 'the', 'i', 'becomes', 'don', 'ie', 'been', 'whereupon', 'seemed', 'most', 'noone', 'whole', 'must', 'cannot', 'per', 'my', 'thereby', 'so', 'he', 'name', 'co', 'its', 'everyone', 'if', 'become', 'thick', 'thus', 'regarding', 'didn', 'give', 'all', 'show', 'any', 'using', 'on', 'further', 'around', 'back', 'least', 'since', 'anyone', 'once', 'can', 'bill', 'hereafter', 'be', 'seems', 'their', 'myself', 'nine', 'also', 'system', 'at', 'more', 'out', 'twelve', 'therein', 'almost', 'except', 'last', 'did', 'something', 'besides', 'via', 'whenever', 'formerly', 'cry', 'one', 'hundred', 'sixty', 'after', 'well', 'them', 'namely', 'empty', 'three', 'even', 'along', 'because', 'ourselves', 'such', 'top', 'due', 'inc', 'themselves', 'ii', 'th', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i','j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'mr', 'us', 'dk', 'workthis', 'youve', 'ive', 'sheve', 'heve', 'weve', 'theyve', 'didnt', 'dont', 'wouldnt', 'couldnt', 'cant', 'shouldnt', 'gl','cm', '________________________________', 'wwwultimatewritercom', 'svd', 'psg', 'zfxpsg', 'editorairsoftpresscom', 'iihf', 'miif','cae']\n",
    "for word in extraWords:\n",
    "    stopwords.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql://localhost/bookisbetter?user=postgres&password=admin')\n",
    "Base = declarative_base()\n",
    "\n",
    "class Book(Base):\n",
    "    __tablename__ = \"Books\"\n",
    "    isbn = Column(String())\n",
    "    asin = Column(String())\n",
    "    average_rating = Column(Float())\n",
    "    description = Column(String())\n",
    "    authors = Column(ARRAY(JSON()))\n",
    "    isbn13 = Column(String())\n",
    "    publication_year = Column(Integer())\n",
    "    image_url = Column(String())\n",
    "    book_id = Column(Integer, primary_key=True)\n",
    "    title = Column(String())\n",
    "    categories =  Column(ARRAY(String()))\n",
    "    \n",
    "    def __init__(self, isbn, asin, average_rating, description, authors, isbn13, publication_year,image_url, book_id, title, categories):\n",
    "        self.isbn = str(isbn)\n",
    "        self.asin = str(asin)\n",
    "        self.average_rating = average_rating\n",
    "        self.description = description\n",
    "        self.authors = authors\n",
    "        self.isbn13 = str(isbn13)\n",
    "        try:\n",
    "            self.publication_year = int(float(publication_year))\n",
    "        except:\n",
    "            self.publication_year = None\n",
    "        self.image_url = str(image_url)\n",
    "        self.book_id = int(book_id)\n",
    "        self.title = title\n",
    "        self.categories = categories\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '%s/%s/%s%s/%s/%s%s/%s/%s%s/%s' % (self.isbn, self.asin, self.average_rating, self.description, self.authors, self.isbn13, self.publication_year, self.image_url, self.book_id, self.title, self.categories)\n",
    "\n",
    "\n",
    "class DescriptionSimilarity(Base):\n",
    "    __tablename__ = \"DescriptionSimilarities\"\n",
    "    book_id = Column(Integer, primary_key=True)\n",
    "    similar_book = Column(Integer, primary_key=True)\n",
    "    cosine = Column(Float())\n",
    "    user_feed = Column(Integer)\n",
    "    last_modified = Column(DateTime())\n",
    "\n",
    "    def __init__(self, book_id, similar_book, cosine, user_feed):\n",
    "        self.book_id = book_id\n",
    "        self.similar_book = similar_book\n",
    "        self.cosine = cosine\n",
    "        self.user_feed = user_feed\n",
    "        self.last_modified = datetime.now()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '%s/%s/%s/%s' % (self.book_id, self.similar_book, self.cosine, self.user_feed)\n",
    "    \n",
    "Base.metadata.create_all(engine) \n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = text.ENGLISH_STOP_WORDS\n",
    "\n",
    "def remove_noise(text):\n",
    "    # Make lowercase\n",
    "    text = text.apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "    \n",
    "    # Remove whitespaces\n",
    "    text = text.apply(lambda x: \" \".join(x.strip() for x in x.split()))\n",
    "    \n",
    "    text = text.replace('\\n', ' ')\n",
    "    \n",
    "    # Remove special characters\n",
    "    text = text.apply(lambda x: \"\".join([\" \" if ord(i) < 32 or ord(i) > 126 else i for i in x]))\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.str.replace('[^\\w\\s]', '')\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = text.str.replace('\\d+', '')\n",
    "    \n",
    "    text = text.str.replace('\\d|[\"]|[,]|[(]|[)]|[$]|[;]|[-]|[.]|[/]|[\\\\]|[?]|[!]|[+]|[[]|[]]', '')\n",
    "    \n",
    "    # Remove Stopwords\n",
    "    text = text.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "    \n",
    "    # Convert to string\n",
    "    text = text.astype(str)\n",
    "        \n",
    "    return text\n",
    "\n",
    "#function to combine title with description, so title will become a keyword as well.\n",
    "def combine_title_and_description(title, desc):\n",
    "    combine = desc + \". \" + title\n",
    "    combine = remove_noise(combine)\n",
    "    return combine\n",
    "\n",
    "def Bert(data):\n",
    "    sbert_model = SentenceTransformer('bert-base-nli-mean-tokens', device='cuda')\n",
    "    return sbert_model.encode(data)\n",
    "\n",
    "def RecommendationEngine(inputDf):\n",
    "    document_embeddings = 0\n",
    "    del document_embeddings\n",
    "    \n",
    "    print(\"cleaning data...\")\n",
    "    data = inputDf\n",
    "    data.columns = [\"Bid\",\"Title\", \"Description\"]\n",
    "    data.loc[data[\"Description\"]==\"\", 'Description'] = data.loc[data['Description']==\"\"]['Title']\n",
    "    data[\"cleaned_desc\"] = combine_title_and_description(data[\"Title\"], data[\"Description\"])\n",
    "    \n",
    "    print(\"tokenizing data...\")\n",
    "    token_desc = []\n",
    "    for index, row in data.iterrows():\n",
    "        token_desc.append(text_to_word_sequence(row[\"cleaned_desc\"]))\n",
    "\n",
    "    filter_token = []\n",
    "    for arr in token_desc:\n",
    "        temp = []\n",
    "        for i in arr:\n",
    "            if not i in stopwords:\n",
    "                temp.append(i)\n",
    "            if len(temp) >= 70:\n",
    "                break\n",
    "        filter_token.append(temp)  \n",
    "    \n",
    "    data['token_desc'] = filter_token\n",
    "    del token_desc\n",
    "    del filter_token\n",
    "    \n",
    "    print(\"lemmatize the tokens and create bag of word...\")\n",
    "    token2string = []\n",
    "    columns = ['token_desc'] \n",
    "    for index, row in data.iterrows():\n",
    "        words = ''\n",
    "        for col in columns:\n",
    "            words += ' '.join(row[col]) + ' '\n",
    "        token2string.append(words)\n",
    "\n",
    "    data['bag_of_word'] = token2string\n",
    "    \n",
    "    data = data.drop(['cleaned_desc'], axis=1)\n",
    "    data = data.drop(['token_desc'], axis=1)\n",
    "\n",
    "    print(\"train bert model with the bag of word...\")\n",
    "\n",
    "    document_embeddings = Bert(data['bag_of_word'])\n",
    "    \n",
    "    del token2string\n",
    "    \n",
    "    return cosine_similarity(document_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function accept up to 3 different use cases \n",
    "# 1 find similarity for whoel dataset\n",
    "# 2 find new similarity based on user feedback\n",
    "# 3 new book with 29999 books to find similarity for it.\n",
    "# to go thru 1 loop is about 5 minutes with GPU\n",
    "# [\"all\", \"missing\", \"improve\", \"new\"]\n",
    "def AllDataPrep():\n",
    "    df = pd.read_sql('select b.book_id, b.title, b.description from public.\"Books\" as b', engine)\n",
    "    indices = []\n",
    "    start = 0\n",
    "    end = 30000\n",
    "    for i in range(int(np.ceil(len(df)/30000))):        \n",
    "        if end in df.index: \n",
    "            indices.append([start, end])\n",
    "        else:\n",
    "            indices.append([start, len(df)])\n",
    "        start += 30000\n",
    "        end += 30000\n",
    "    del df\n",
    "\n",
    "    return indices\n",
    "\n",
    "def ImproveDataPrep():\n",
    "    df = pd.read_sql(\"\"\"SELECT * FROM public.\"DescriptionSimilarities\" WHERE   last_modified >= NOW() - '21 hour'::INTERVAL\"\"\", engine)\n",
    "    modified_title = pd.Series(df['book_id']).drop_duplicates() #we only need to get new recommendation for these books because there are the last modified books\n",
    "    modified_title = modified_title.reset_index(drop=True)\n",
    "    modified_title = list(modified_title)\n",
    "    book_id_list = [] # create a list containing modiefied title and the similar book. \n",
    "    for index, row in df.iterrows():\n",
    "        if row[\"book_id\"] not in book_id_list:\n",
    "            book_id_list.append(row[\"book_id\"])\n",
    "        if row[\"similar_book\"] not in book_id_list:\n",
    "            book_id_list.append(row[\"similar_book\"])\n",
    "\n",
    "    limit = 10000 - (len(book_id_list) % 10000) - 1\n",
    "    random_id_list = pd.read_sql(f\"\"\"SELECT book_id FROM public.\"Books\" ORDER BY RANDOM() LIMIT {limit}\"\"\" ,engine)\n",
    "    random_id_list = list(random_id_list[\"book_id\"])\n",
    "    for i in book_id_list:\n",
    "        if i not in random_id_list:\n",
    "            random_id_list.append(i)\n",
    "\n",
    "    if len(random_id_list) > 10000:\n",
    "        engine_list = [ list(random_id_list[i:i + 10000]) for i in range(0, len(random_id_list), 10000)]\n",
    "        print(len(engine_list))\n",
    "    else:\n",
    "        engine_list = random_id_list\n",
    "    \n",
    "    return engine_list, modified_title\n",
    "    \n",
    "    \n",
    "def NewDataPrep():\n",
    "    return 3\n",
    "\n",
    "def RecommendationInitialization(useCase):\n",
    "    print(\"preparing data...\")\n",
    "    if useCase == \"All\":\n",
    "        data = AllDataPrep()\n",
    "        for i in data:\n",
    "            print(\"batch between index: \" + str(i[0]) + \" and \" + str(i[1]) + \"...\")\n",
    "            df = pd.read_sql('select b.book_id, b.title, b.description from public.\"Books\" as b', engine)[i[0]:i[1]]\n",
    "            df = df.reset_index(drop=True)\n",
    "            GetRecommendation(df, pd.Series(df['book_id']))\n",
    "        \n",
    "    elif useCase == \"Update\":\n",
    "        data, target_id = ImproveDataPrep()\n",
    "        if any(isinstance(i, list) for i in data):\n",
    "            print(\"data: multiple batches...\")\n",
    "            for i in data:\n",
    "                df = pd.read_sql(f\"\"\"select b.book_id, b.title, b.description from public.\"Books\" as b where b.book_id IN {tuple(i)}\"\"\" ,engine)\n",
    "                df = df.reset_index(drop=True)\n",
    "                GetRecommendation(df, pd.Series(df['book_id']), target_id)\n",
    "        else:\n",
    "            df = pd.read_sql(f\"\"\"select b.book_id, b.title, b.description from public.\"Books\" as b where b.book_id IN {tuple(data)}\"\"\" ,engine)\n",
    "            df = df.reset_index(drop=True)\n",
    "            GetRecommendation(df, pd.Series(df['book_id']), target_id)\n",
    "    elif useCase == \"New\":\n",
    "        data = NewDataPrep() \n",
    "        print(data)  \n",
    "    \n",
    "\n",
    "def GetRecommendation(data, indices, target_id=[]):\n",
    "    cosine_similarity = RecommendationEngine(data)\n",
    "    getTop20(indices, cosine_similarity)\n",
    "    if len(target_id) == 0:\n",
    "        getTop20ForAll(indices, cosine_similarity)\n",
    "    else:\n",
    "        getTop20ForSpecific(indices, cosine_similarity, target_id)\n",
    "    \n",
    "def getTop20ForAll(indices, cosine_similarity): \n",
    "    print(\"find similarity and inserting to database...\")\n",
    "    for i in indices:\n",
    "        idx = indices[indices == i].index[0]\n",
    "        score_series = pd.Series(cosine_similarity[idx]).sort_values(ascending = False)[1:21]\n",
    "\n",
    "        for index, value in score_series.items():\n",
    "            sim_id = indices[index].item() \n",
    "            sim_cosine = value\n",
    "            qry = session.query(DescriptionSimilarity).filter(DescriptionSimilarity.book_id == i, DescriptionSimilarity.similar_book == int(sim_id)).first()\n",
    "            if not qry:\n",
    "                session.add(DescriptionSimilarity(book_id=i, similar_book= int(sim_id), cosine=float(sim_cosine), user_feed= int(0)))\n",
    "        session.commit()\n",
    "\n",
    "def getTop20ForSpecific(indices, cosine_similarity, target_id): \n",
    "    print(\"find similarity and inserting to database...\")\n",
    "    for i in indices:\n",
    "        if i in target_id:\n",
    "            idx = indices[indices == i].index[0]\n",
    "            score_series = pd.Series(cosine_similarity[idx]).sort_values(ascending = False)[1:11]\n",
    "\n",
    "            for index, value in score_series.items():\n",
    "                sim_id = indices[index].item() \n",
    "                sim_cosine = value                \n",
    "                qry = session.query(DescriptionSimilarity).filter(DescriptionSimilarity.book_id == i, DescriptionSimilarity.similar_book == int(sim_id)).first()\n",
    "                if not qry:\n",
    "                    session.add(DescriptionSimilarity(book_id=i, similar_book= int(sim_id), cosine=float(sim_cosine), user_feed= int(0)))\n",
    "                else:\n",
    "                    if sim_cosine > qry.cosine:\n",
    "                        qry.cosine = sim_cosine\n",
    "#                         qry.last_modified = datetime.now()\n",
    "            session.commit()\n",
    "\n",
    "def GetRecommendation(data, indices, target_id=[]):\n",
    "    cosine_similarity = RecommendationEngine(data)\n",
    "    if len(target_id) == 0:\n",
    "        getTop20ForAll(indices, cosine_similarity)\n",
    "    else:\n",
    "        getTop20ForSpecific(indices, cosine_similarity, target_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing data...\n",
      "cleaning data...\n",
      "tokenizing data...\n",
      "lemmatize the tokens and create bag of word...\n",
      "train bert model with the bag of word...\n",
      "target_id\n",
      "find similarity and inserting to database...\n"
     ]
    }
   ],
   "source": [
    "RecommendationInitialization(\"Update\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
