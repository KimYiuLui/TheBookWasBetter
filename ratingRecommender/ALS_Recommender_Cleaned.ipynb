{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# spark imports\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col, lower\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlsRecommender:\n",
    "    \"\"\"\n",
    "    This a collaborative filtering recommender with Alternating Least Square\n",
    "    Matrix Factorization, which is implemented by Spark\n",
    "    \"\"\"\n",
    "    def __init__(self, spark_session, path_books, path_ratings, path_toread):\n",
    "        self.spark = spark_session\n",
    "        self.sc = spark_session.sparkContext\n",
    "        self.booksDF = self._load_file(path_books) \\\n",
    "            .select(['id', 'book_id', 'best_book_id', 'books_count', 'isbn13', 'authors', 'original_publication_year', 'original_title', 'average_rating', 'ratings_count', 'work_ratings_count', 'work_text_reviews_count', 'image_url'])\n",
    "        self.ratingsDF = self._load_file(path_ratings) \\\n",
    "            .select(['book_id', 'user_id', 'rating'])\n",
    "        self.toReadsDF = self._load_file(path_toread) \\\n",
    "            .select(['user_id', 'book_id'])\n",
    "        self.model = ALS(\n",
    "            userCol='user_id',\n",
    "            itemCol='book_id',\n",
    "            ratingCol='rating',\n",
    "            coldStartStrategy=\"drop\",\n",
    "            nonnegative=True,\n",
    "            implicitPrefs = False)\n",
    "        \n",
    "    def _load_file(self, filepath):\n",
    "        \"\"\"\n",
    "        load csv file into memory as spark DF\n",
    "        \"\"\"\n",
    "        return self.spark.read.load(filepath, format='csv',\n",
    "                                    header=True, inferSchema=True)\n",
    "    \n",
    "    def tune_model(self, maxIter, regParams, ranks, split_ratio=(6, 2, 2)):\n",
    "        \"\"\"\n",
    "        Hyperparameter tuning for ALS model\n",
    "        Parameters\n",
    "        ----------\n",
    "        maxIter: int, max number of learning iterations\n",
    "        regParams: list of float, regularization parameter\n",
    "        ranks: list of float, number of latent factors\n",
    "        split_ratio: tuple, (train, validation, test)\n",
    "        \"\"\"\n",
    "        # split data\n",
    "        train, val, test = self.ratingsDF.randomSplit(split_ratio)\n",
    "        # holdout tuning\n",
    "        self.model = tune_ALS(self.model, train, val,\n",
    "                              maxIter, regParams, ranks)\n",
    "        # test model\n",
    "        predictions = self.model.transform(test)\n",
    "        evaluator = RegressionEvaluator(metricName=\"rmse\",\n",
    "                                        labelCol=\"rating\",\n",
    "                                        predictionCol=\"prediction\")\n",
    "        rmse = evaluator.evaluate(predictions)\n",
    "        print('The out-of-sample RMSE of the best tuned model is:', rmse)\n",
    "        # clean up\n",
    "        del train, val, test, predictions, evaluator\n",
    "        gc.collect()   \n",
    "    \n",
    "    def set_model_params(self, maxIter, regParam, rank):\n",
    "        \"\"\"\n",
    "        set model params for pyspark.ml.recommendation.ALS\n",
    "        Parameters\n",
    "        ----------\n",
    "        maxIter: int, max number of learning iterations\n",
    "        regParams: float, regularization parameter\n",
    "        ranks: float, number of latent factors\n",
    "        \"\"\"\n",
    "        self.model = self.model \\\n",
    "            .setMaxIter(maxIter) \\\n",
    "            .setRank(rank) \\\n",
    "            .setRegParam(regParam)\n",
    "        \n",
    "    def _regex_matching(self, fav_book):\n",
    "        \"\"\"\n",
    "        return the closest matches via SQL regex.\n",
    "        If no match found, return None\n",
    "        Parameters\n",
    "        ----------\n",
    "        fav_book: str, name of user input book\n",
    "        Return\n",
    "        ------\n",
    "        list of indices of the matching books\n",
    "        \"\"\"\n",
    "        print('You have input book:', fav_book)\n",
    "        matchesDF = self.booksDF \\\n",
    "            .filter(\n",
    "                lower(\n",
    "                    col('original_title')\n",
    "                ).like('%{}%'.format(fav_book.lower()))\n",
    "            ) \\\n",
    "            .select('book_id', 'original_title')\n",
    "        if not len(matchesDF.take(1)):\n",
    "            print('Oops! No match is found')\n",
    "        else:\n",
    "            bookIds = matchesDF.rdd.map(lambda r: r[0]).collect()\n",
    "            titles = matchesDF.rdd.map(lambda r: r[1]).collect()\n",
    "            print('Found possible matches in our database: '\n",
    "                  '{0}\\n'.format([x for x in titles]))\n",
    "            return bookIds\n",
    "        \n",
    "    def _append_ratings(self, userId, bookIds):\n",
    "        \"\"\"\n",
    "        append a user's book ratings to ratingsDF\n",
    "        Parameter\n",
    "        ---------\n",
    "        userId: int, userId of a user\n",
    "        bookIds: int, bookIds of user's favorite books\n",
    "        \"\"\"\n",
    "        # create new user rdd\n",
    "        user_rdd = self.sc.parallelize(\n",
    "            [(userId, bookId, 5.0) for bookId in bookIds])\n",
    "        # transform to user rows\n",
    "        user_rows = user_rdd.map(\n",
    "            lambda x: Row(\n",
    "                bookId=int(x[0]),\n",
    "                userId=int(x[1]),\n",
    "                rating=float(x[2])\n",
    "            )\n",
    "        )\n",
    "        # transform rows to spark DF\n",
    "        userDF = self.spark.createDataFrame(user_rows) \\\n",
    "            .select(self.ratingsDF.columns)\n",
    "        # append to ratingsDF\n",
    "        self.ratingsDF = self.ratingsDF.union(userDF)\n",
    "        \n",
    "    def _create_inference_data(self, userId, bookIds):\n",
    "        \"\"\"\n",
    "        create a user with all books except ones were rated for inferencing\n",
    "        \"\"\"\n",
    "        # filter books\n",
    "        other_bookIds = self.booksDF \\\n",
    "            .filter(~col('book_id').isin(bookIds)) \\\n",
    "            .select(['book_id']) \\\n",
    "            .rdd.map(lambda r: r[0]) \\\n",
    "            .collect()\n",
    "        # create inference rdd\n",
    "        inferenceRDD = self.sc.parallelize(\n",
    "            [(userId, bookId) for bookId in other_bookIds]\n",
    "        ).map(\n",
    "            lambda x: Row(\n",
    "                userId=int(x[0]),\n",
    "                bookId=int(x[1]),\n",
    "            )\n",
    "        )\n",
    "        # transform to inference DF\n",
    "        inferenceDF = self.spark.createDataFrame(inferenceRDD) \\\n",
    "            .select(['userId', 'bookId'])\n",
    "        return inferenceDF\n",
    "    \n",
    "    def _inference(self, model, fav_book, n_recommendations):\n",
    "        \"\"\"\n",
    "        return top n book recommendations based on user's input book\n",
    "        Parameters\n",
    "        ----------\n",
    "        model: spark ALS model\n",
    "        fav_book: str, name of user input book\n",
    "        n_recommendations: int, top n recommendations\n",
    "        Return\n",
    "        ------\n",
    "        list of top n similar book recommendations\n",
    "        \"\"\"\n",
    "        # create a userId\n",
    "        userId = self.ratingsDF.agg({\"userId\": \"max\"}).collect()[0][0] + 1\n",
    "        # get bookIds of favorite books\n",
    "        bookIds = self._regex_matching(fav_book)\n",
    "        # append new user with his/her ratings into data\n",
    "        self._append_ratings(userId, bookIds)\n",
    "        # matrix factorization\n",
    "        model = model.fit(self.ratingsDF)\n",
    "        # get data for inferencing\n",
    "        inferenceDF = self._create_inference_data(userId, bookIds)\n",
    "        # make inference\n",
    "        return model.transform(inferenceDF) \\\n",
    "            .select(['bookId', 'prediction']) \\\n",
    "            .orderBy('prediction', ascending=False) \\\n",
    "            .rdd.map(lambda r: (r[0], r[1])) \\\n",
    "            .take(n_recommendations)\n",
    "    \n",
    "    def make_recommendations(self, fav_book, n_recommendations):\n",
    "        \"\"\"\n",
    "        make top n book recommendations\n",
    "        Parameters\n",
    "        ----------\n",
    "        fav_book: str, name of user input book\n",
    "        n_recommendations: int, top n recommendations\n",
    "        \"\"\"\n",
    "        # make inference and get raw recommendations\n",
    "        print('Recommendation system start to make inference ...')\n",
    "        t0 = time.time()\n",
    "        raw_recommends = \\\n",
    "            self._inference(self.model, fav_book, n_recommendations)\n",
    "        bookIds = [r[0] for r in raw_recommends]\n",
    "        scores = [r[1] for r in raw_recommends]\n",
    "        print('It took my system {:.2f}s to make inference \\n\\\n",
    "              '.format(time.time() - t0))\n",
    "        # get book titles\n",
    "        book_titles = self.booksDF \\\n",
    "            .filter(col('bookId').isin(bookIds)) \\\n",
    "            .select('title') \\\n",
    "            .rdd.map(lambda r: r[0]) \\\n",
    "            .collect()\n",
    "        # print recommendations\n",
    "        print('Recommendations for {}:'.format(fav_book))\n",
    "        for i in range(len(book_titles)):\n",
    "            print('{0}: {1}, with rating '\n",
    "                  'of {2}'.format(i+1, book_titles[i], scores[i]))\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    \"\"\"\n",
    "    data object make loading raw files easier\n",
    "    \"\"\"\n",
    "    def __init__(self, spark_session, filepath):\n",
    "        \"\"\"\n",
    "        spark dataset constructor\n",
    "        \"\"\"\n",
    "        self.spark = spark_session\n",
    "        self.sc = spark_session.sparkContext\n",
    "        self.filepath = filepath\n",
    "        # build spark data object\n",
    "        self.RDD = self.load_file_as_RDD(self.filepath)\n",
    "        self.DF = self.load_file_as_DF(self.filepath)\n",
    "        \n",
    "    def load_file_as_RDD(self, filepath):\n",
    "        ratings_RDD = self.sc.textFile(filepath)\n",
    "        header = ratings_RDD.take(1)[0]\n",
    "        return ratings_RDD \\\n",
    "            .filter(lambda line: line != header) \\\n",
    "            .map(lambda line: line.split(\",\")) \\\n",
    "            .map(lambda tokens: (int(tokens[0]), int(tokens[1]), float(tokens[2]))) # noqa\n",
    "\n",
    "    def load_file_as_DF(self, filepath):\n",
    "        ratings_RDD = self.load_file_as_rdd(filepath)\n",
    "        ratingsRDD = ratings_RDD.map(lambda tokens: Row(\n",
    "            userId=int(tokens[0]), bookId=int(tokens[1]), rating=float(tokens[2]))) # noqa\n",
    "        return self.spark.createDataFrame(ratingsRDD)\n",
    "\n",
    "def tune_ALS(model, train_data, validation_data, maxIter, regParams, ranks):\n",
    "    \"\"\"\n",
    "    grid search function to select the best model based on RMSE of\n",
    "    validation data\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: spark ML model, ALS\n",
    "    train_data: spark DF with columns ['user_id', 'book_id', 'rating']\n",
    "    validation_data: spark DF with columns ['user_id', 'book_id', 'rating']\n",
    "    maxIter: int, max number of learning iterations\n",
    "    regParams: list of float, one dimension of hyper-param tuning grid\n",
    "    ranks: list of float, one dimension of hyper-param tuning grid\n",
    "    Return\n",
    "    ------\n",
    "    The best fitted ALS model with lowest RMSE score on validation data\n",
    "    \"\"\"\n",
    "    # initial\n",
    "    min_error = float('inf')\n",
    "    best_rank = -1\n",
    "    best_regularization = 0\n",
    "    best_model = None\n",
    "    for rank in ranks:\n",
    "        for reg in regParams:\n",
    "            # get ALS model\n",
    "            als = model.setMaxIter(maxIter).setRank(rank).setRegParam(reg)\n",
    "            # train ALS model\n",
    "            model = als.fit(train_data)\n",
    "            # evaluate the model by computing the RMSE on the validation data\n",
    "            predictions = model.transform(validation_data)\n",
    "            evaluator = RegressionEvaluator(metricName=\"rmse\",\n",
    "                                            labelCol=\"rating\",\n",
    "                                            predictionCol=\"prediction\")\n",
    "            rmse = evaluator.evaluate(predictions)\n",
    "            print('{} latent factors and regularization = {}: '\n",
    "                  'validation RMSE is {}'.format(rank, reg, rmse))\n",
    "            if rmse < min_error:\n",
    "                min_error = rmse\n",
    "                best_rank = rank\n",
    "                best_regularization = reg\n",
    "                best_model = model\n",
    "    print('\\nThe best model has {} latent factors and '\n",
    "          'regularization = {}'.format(best_rank, best_regularization))\n",
    "    return best_model        \n",
    "        \n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        prog=\"Rating Recommender\",\n",
    "        description=\"Run ALS Rating Recommender\")\n",
    "    parser.add_argument('--path', nargs='?', default='../ratingRecommender',\n",
    "                        help='input data path')\n",
    "    parser.add_argument('--books_filename', nargs='?', default='dataset\\\\cleaned_data_books_version_3.csv.csv',\n",
    "                        help='provide books filename')\n",
    "    parser.add_argument('--ratings_filename', nargs='?', default='dataset\\\\ratings.csv',\n",
    "                        help='provide ratings filename')\n",
    "    parser.add_argument('--toread_filename', nargs='?', default='dataset\\\\to_read.csv',\n",
    "                        help='provide to read filename')\n",
    "    parser.add_argument('--book_name', nargs='?', default='',\n",
    "                        help='provide a recommended book name')\n",
    "    parser.add_argument('--top_n', type=int, default=10,\n",
    "                        help='top n book recommendations')\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: Rating Recommender [-h] [--path [PATH]]\n",
      "                          [--books_filename [BOOKS_FILENAME]]\n",
      "                          [--ratings_filename [RATINGS_FILENAME]]\n",
      "                          [--toread_filename [TOREAD_FILENAME]]\n",
      "                          [--book_name [BOOK_NAME]] [--top_n TOP_N]\n",
      "Rating Recommender: error: unrecognized arguments: -f C:\\Users\\lou-a\\AppData\\Roaming\\jupyter\\runtime\\kernel-787e9e62-2731-426f-9ede-db8b56a6953b.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # get args\n",
    "    args = parse_args()\n",
    "    data_path = args.path\n",
    "    books_filename = args.books_filename\n",
    "    ratings_filename = args.ratings_filename\n",
    "    toread_filename = args.toread_filename\n",
    "    book_name = args.movie_name\n",
    "    top_n = args.top_n\n",
    "    # initial spark\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Rating recommender\") \\\n",
    "        .getOrCreate()\n",
    "    # initial recommender system\n",
    "    recommender = AlsRecommender(\n",
    "        spark,\n",
    "        os.path.join(data_path, movies_filename),\n",
    "        os.path.join(data_path, ratings_filename),\n",
    "        os.path.join(data_path, toread_filename))\n",
    "    # set params\n",
    "    recommender.set_model_params(10, 0.05, 20)\n",
    "    # make recommendations\n",
    "    recommender.make_recommendations(movie_name, top_n)\n",
    "    # stop\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 - MININTEL",
   "language": "python",
   "name": "python3 - minintel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
