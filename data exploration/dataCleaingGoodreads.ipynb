{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First split it into workable chunks\n",
    "You need the .gz files from https://sites.google.com/eng.ucsd.edu/ucsdbookgraph/home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import unicodedata as ud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to delete all unnecessary columns and select all english books of \"goodreads_books.json.gz\" \n",
    "lang = ['en', 'en-CA', 'en-US', 'en-GB', 'eng']\n",
    "toDel = ['country_code', 'is_ebook', 'kindle_asin',  'link', 'url','publication_day', 'publication_month', 'num_pages', 'popular_shelves', 'publisher', 'title_without_series', 'format']\n",
    "\n",
    "def load_data(file_name, start, end):\n",
    "    count = 0\n",
    "    data = []\n",
    "    with gzip.open(file_name) as fin:\n",
    "        for l in fin:\n",
    "            if count < start:\n",
    "                count += 1\n",
    "                continue\n",
    "            elif count >= end:\n",
    "                break\n",
    "            else:\n",
    "                d = json.loads(l)\n",
    "                if d['language_code'] in lang:\n",
    "                    for element in toDel:\n",
    "                        d.pop(element)\n",
    "                    data.append(d)\n",
    "                count += 1\n",
    "        print(len(data))\n",
    "        return data\n",
    "DIR = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# books = load_data(os.path.join(DIR, 'goodreads_books.json.gz'),0,500000)\n",
    "# books = load_data(os.path.join(DIR, 'goodreads_books.json.gz'),500000,1000000)\n",
    "# books = load_data(os.path.join(DIR, 'goodreads_books.json.gz'),1000000,1500000)\n",
    "# books = load_data(os.path.join(DIR, 'goodreads_books.json.gz'),1500000,2000000)\n",
    "# books = load_data(os.path.join(DIR, 'goodreads_books.json.gz'),2000000, 2370000)\n",
    "books = load_data(os.path.join(DIR, 'goodreads_books.json.gz'),0, 2370000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../GoodReadsDatagoodreadsBooks.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(books, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genre and Authors don't need to be split so lets just store it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_data(file_name):\n",
    "    data = []\n",
    "    with gzip.open(file_name) as fin:\n",
    "        for l in fin:\n",
    "            d = json.loads(l)\n",
    "            data.append(d)\n",
    "            \n",
    "        print(len(data))\n",
    "        return data\n",
    "DIR = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre = load_all_data(os.path.join(DIR, 'goodreads_book_genres_initial.json.gz'))\n",
    "with open('../../GoodReadsDatagoodreadsGenre.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(genre, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author = load_all_data(os.path.join(DIR, 'goodreads_book_authors.json.gz'))\n",
    "with open('../../GoodReadsDatagoodreadsAuthors.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(author, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we split the 15+ million reviews into 4 parts of 4 million"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "739967\n"
     ]
    }
   ],
   "source": [
    "toDel = ['review_text', 'date_added', 'date_updated', 'read_at', 'started_at', 'n_votes', 'n_comments', 'review_id']\n",
    "\n",
    "def load_data(file_name, start, end):\n",
    "    count = 0\n",
    "    data = []\n",
    "    with gzip.open(file_name) as fin:\n",
    "        for l in fin:\n",
    "            if count < start:\n",
    "                count += 1\n",
    "                continue\n",
    "            elif count >= end:\n",
    "                break\n",
    "            else:\n",
    "                d = json.loads(l)\n",
    "                for element in toDel:\n",
    "                    d.pop(element)\n",
    "                data.append(d)\n",
    "                count += 1\n",
    "        print(len(data))\n",
    "        return data\n",
    "DIR = './'\n",
    "\n",
    "review = load_data(os.path.join(DIR, 'goodreads_reviews_dedup.json.gz'),0, 4000000)\n",
    "review = load_data(os.path.join(DIR, 'goodreads_reviews_dedup.json.gz'),4000000, 8000000)\n",
    "review = load_data(os.path.join(DIR, 'goodreads_reviews_dedup.json.gz'),8000000, 12000000)\n",
    "review = load_data(os.path.join(DIR, 'goodreads_reviews_dedup.json.gz'),12000000, 16000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../GoodReadsData/goodreadsReview1.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(review, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now lets clean each part of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = pd.read_json(\"../../GoodReadsData/goodreadsBooks.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting rid of books without title which also don't have other datas \n",
    "books = books[books['title'] != \"\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#detect for latin words in title. because there are other language books labeled as english\n",
    "latin_letters= {}\n",
    "\n",
    "def is_latin(uchr):\n",
    "    try: return latin_letters[uchr]\n",
    "    except KeyError:\n",
    "         return latin_letters.setdefault(uchr, 'LATIN' in ud.name(uchr))\n",
    "\n",
    "def only_roman_chars(unistr):\n",
    "    return all(is_latin(uchr)\n",
    "           for uchr in unistr\n",
    "           if uchr.isalpha())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in books.iterrows():\n",
    "    if only_roman_chars(row[\"title\"]) != True:\n",
    "        books.drop(index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = books.reset_index(drop=True)\n",
    "#By sorting in decending the duplicates with empty isbn or isbn13 will be under the ones with isbn or isbn13 \n",
    "#So the one without those code will be flagged as duplicates\n",
    "books = books.sort_values(by=['isbn', 'isbn13','title'],ascending=False)\n",
    "\n",
    "# remove duplicates, it is possbile of two same title name with different publication_year\n",
    "books = books.drop_duplicates(subset=['publication_year','title'])\n",
    "print(\"Length after deleting duplicate: \" + str(len(books)))\n",
    "books = books.sort_values(by=['title'])\n",
    "books = books.reset_index(drop=True)\n",
    "books.pop(\"language_code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depends on your memory usage you can store it into a new json file \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books.to_json('../../GoodReadsData/goodreadsBooks.json',orient='records')\n",
    "#restart kernel en run the imports block and then continue from this next block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = pd.read_json('../../GoodReadsData/goodreadsBooks.json')\n",
    "bookIds = books['book_id'].to_list()\n",
    "\n",
    "### OR ###\n",
    "\n",
    "# bookIds = books['book_id'].to_list()\n",
    "\n",
    "# lets drop the bookids in genre that doen't exist anymore in book.json\n",
    "genres = pd.read_json(\"../../GoodReadsData/goodreadsGenre.json\")\n",
    "genres = genres[genres['book_id'].isin(bookIds)]\n",
    "genres = genres.reset_index(drop=True)\n",
    "\n",
    "#then lets find all the books without genres\n",
    "noGenre = genres[genres['genres']=={}]\n",
    "noGenre = noGenre[\"book_id\"].to_list()\n",
    "\n",
    "#remove every book without genre. we would scrape them but with the given time it is impposible\n",
    "books = books[~books['book_id'].isin(noGenre)]\n",
    "books = books.reset_index(drop=True)\\\n",
    "books = books.sort_values(by=['book_id'])\n",
    "\n",
    "genres = genres[genres['genres']!={}]\n",
    "genres = genres.sort_values(by=['book_id'])\n",
    "\n",
    "# this might be an unneeded step but it is easier for project mate to make use of the genre for ML\n",
    "genresOnly = genres['genres'].tolist()\n",
    "newGenre = []\n",
    "for obj in genresOnly:\n",
    "    temp = []\n",
    "    for key in obj:\n",
    "        if \", \" in key:\n",
    "            nested = key.split(\", \")\n",
    "            for single in nested:\n",
    "                temp.append(single)\n",
    "        else:\n",
    "            temp.append(key)\n",
    "    newGenre.append(temp)\n",
    "    \n",
    "genres[\"categories\"]  = newGenre\n",
    "genres = genres.drop(['genres'], axis=1)\n",
    "\n",
    "books['categories'] = newGenre\n",
    "\n",
    "genres.to_json('../../GoodReadsData/goodreadsGenre.json',orient='records')\n",
    "books.to_json('../../GoodReadsData/goodreadsBooks.json',orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depends on your memory usage you might wanna reset kernel run the import block and work after this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets clean the reviews since we reduce 2million books to 700k books there should be also less reviews\n",
    "\n",
    "books = pd.read_json('../../GoodReadsData/goodreadsBooks.json')\n",
    "bookIds = books['book_id'].to_list()\n",
    "\n",
    "### OR ###\n",
    "\n",
    "# bookIds = books['book_id'].to_list()\n",
    "\n",
    "review1 = pd.read_json(\"../../GoodReadsData/goodreadsReview1.json\")\n",
    "review2 = pd.read_json(\"../../GoodReadsData/goodreadsReview2.json\")\n",
    "review3 = pd.read_json(\"../../GoodReadsData/goodreadsReview3.json\")\n",
    "review4 = pd.read_json(\"../../GoodReadsData/goodreadsReview4.json\")\n",
    "\n",
    "review1 = review1[review1[\"book_id\"].isin(bookid)]\n",
    "review2 = review2[review2[\"book_id\"].isin(bookid)]\n",
    "review3 = review3[review3[\"book_id\"].isin(bookid)]\n",
    "review4 = review4[review4[\"book_id\"].isin(bookid)]\n",
    "\n",
    "reviews = review1.append(review1 ,ignore_index = True)\n",
    "reviews = reviews.append(review2 ,ignore_index = True)\n",
    "reviews = reviews.append(review3 ,ignore_index = True)\n",
    "reviews = reviews.append(review4 ,ignore_index = True)\n",
    "\n",
    "reviews.to_json(\"../../GoodReadsData/goodreadsReviews.json\", orient='records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
